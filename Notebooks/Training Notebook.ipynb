{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import the modules","metadata":{}},{"cell_type":"markdown","source":"### Basic imports","metadata":{}},{"cell_type":"code","source":"import os\nfrom tqdm.notebook import tqdm\n\nimport numpy as np \nimport pandas as pd \nfrom collections import Counter\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nimport PIL\nfrom PIL import Image\nfrom IPython.display import display\n\nimport openslide\nimport skimage.io\n\nfrom sklearn.model_selection import train_test_split\n\nimport cv2\nimport zipfile","metadata":{"execution":{"iopub.status.busy":"2021-09-10T08:10:11.376582Z","iopub.execute_input":"2021-09-10T08:10:11.376901Z","iopub.status.idle":"2021-09-10T08:10:12.717365Z","shell.execute_reply.started":"2021-09-10T08:10:11.376857Z","shell.execute_reply":"2021-09-10T08:10:12.71653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model based imports","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\nfrom keras import Model\nfrom keras.layers import Input, Dropout, concatenate, Dense\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D\n\nimport keras.backend as K\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers","metadata":{"execution":{"iopub.status.busy":"2021-09-10T08:10:24.878859Z","iopub.execute_input":"2021-09-10T08:10:24.879218Z","iopub.status.idle":"2021-09-10T08:10:24.884354Z","shell.execute_reply.started":"2021-09-10T08:10:24.879187Z","shell.execute_reply":"2021-09-10T08:10:24.883353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data folders","metadata":{}},{"cell_type":"code","source":"BASE_DIR = '../input/prostate-cancer-grade-assessment/'\nSAVE_DIR = '/kaggle/working/'\n\nOVERLAY_IMG_DIR = '/kaggle/working/overlay/'\nTILING_IMG_DIR = '/kaggle/working/tiled_images/'\n\nIMG_SIZE = 224","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:13:05.929741Z","iopub.execute_input":"2021-09-10T05:13:05.930076Z","iopub.status.idle":"2021-09-10T05:13:05.937416Z","shell.execute_reply.started":"2021-09-10T05:13:05.93004Z","shell.execute_reply":"2021-09-10T05:13:05.936661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Dataset","metadata":{}},{"cell_type":"code","source":"base_data = pd.read_csv(BASE_DIR+'train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:13:05.938945Z","iopub.execute_input":"2021-09-10T05:13:05.939331Z","iopub.status.idle":"2021-09-10T05:13:05.972915Z","shell.execute_reply.started":"2021-09-10T05:13:05.939293Z","shell.execute_reply":"2021-09-10T05:13:05.972053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Filtering non-mask images","metadata":{}},{"cell_type":"code","source":"images_without_mask = []\nfor image in base_data['image_id']:\n    if not os.path.exists(BASE_DIR+'train_label_masks/'+image+'_mask.tiff'):\n        images_without_mask.append(image)\n\ndata_without_mask = base_data[base_data['image_id'].isin(images_without_mask)]\n\nbase_data = base_data[~base_data['image_id'].isin(images_without_mask)]","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:13:07.398031Z","iopub.execute_input":"2021-09-10T05:13:07.39841Z","iopub.status.idle":"2021-09-10T05:13:40.622606Z","shell.execute_reply.started":"2021-09-10T05:13:07.398377Z","shell.execute_reply":"2021-09-10T05:13:40.621758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Separate the data sources","metadata":{}},{"cell_type":"code","source":"radboud_train_set = base_data[base_data['data_provider']=='radboud']\nkarolinska_train_set = base_data[base_data['data_provider']=='karolinska']","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:13:40.624203Z","iopub.execute_input":"2021-09-10T05:13:40.624673Z","iopub.status.idle":"2021-09-10T05:13:40.635972Z","shell.execute_reply.started":"2021-09-10T05:13:40.624633Z","shell.execute_reply":"2021-09-10T05:13:40.635201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 1: Segmentation Model","metadata":{}},{"cell_type":"markdown","source":"### Split and load the data for the segmentation model","metadata":{}},{"cell_type":"code","source":"def load_images(image, with_mask=True):\n    \n    slide = openslide.OpenSlide(os.path.join(BASE_DIR+\"train_images\", f'{image}.tiff'))\n    \n    spacing = 1 / (float(slide.properties['tiff.XResolution']) / 10000)\n    img = slide.get_thumbnail(size=(IMG_SIZE,IMG_SIZE))\n    \n    \n    img = Image.fromarray(np.array(img))\n    img = img.resize((IMG_SIZE, IMG_SIZE))\n    img = np.array(img)\n\n    if with_mask:\n        mask =  openslide.OpenSlide(os.path.join(BASE_DIR+'train_label_masks', f'{image}_mask.tiff'))\n        mask_data = mask.read_region((0,0), mask.level_count - 1, mask.level_dimensions[-1])\n\n        mask_data = Image.fromarray(np.array(mask_data))\n        mask_data = mask_data.resize((IMG_SIZE, IMG_SIZE))\n        mask_data = np.array(mask_data)\n        mask_data = mask_data/5\n    \n        return img, mask_data[:,:,0]\n\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:13:40.637768Z","iopub.execute_input":"2021-09-10T05:13:40.638101Z","iopub.status.idle":"2021-09-10T05:13:40.648234Z","shell.execute_reply.started":"2021-09-10T05:13:40.638067Z","shell.execute_reply":"2021-09-10T05:13:40.647369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unet_train_ids, unet_test_ids, unet_train_labels, unet_test_labels = train_test_split(radboud_train_set['image_id'], radboud_train_set['isup_grade'], train_size=0.85)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:13:40.649901Z","iopub.execute_input":"2021-09-10T05:13:40.650292Z","iopub.status.idle":"2021-09-10T05:13:40.658854Z","shell.execute_reply.started":"2021-09-10T05:13:40.650255Z","shell.execute_reply":"2021-09-10T05:13:40.658058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss metric: Dice coefficient","metadata":{}},{"cell_type":"code","source":"def Dice_coeff(y_true, y_pred):\n    smooth = 1\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:13:40.660078Z","iopub.execute_input":"2021-09-10T05:13:40.660504Z","iopub.status.idle":"2021-09-10T05:13:40.666979Z","shell.execute_reply.started":"2021-09-10T05:13:40.660458Z","shell.execute_reply":"2021-09-10T05:13:40.666042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### UNet model","metadata":{}},{"cell_type":"code","source":"def unet_model():\n    in1 = Input(shape=(IMG_SIZE, IMG_SIZE, 3 ))\n\n    conv1 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(in1)\n    conv1 = Dropout(0.3)(conv1)\n    conv1 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv1)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n\n    conv2 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool1)\n    conv2 = Dropout(0.3)(conv2)\n    conv2 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv2)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n\n    conv3 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool2)\n    conv3 = Dropout(0.3)(conv3)\n    conv3 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv3)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    \n    \n    conv4 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool3)\n    conv4 = Dropout(0.3)(con43)\n    conv4 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv4)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    \n    conv5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool4)\n    conv5 = Dropout(0.3)(conv5)\n    conv5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv5)\n    up1 = concatenate([UpSampling2D((2, 2))(conv5), conv4], axis=-1)\n\n    conv6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool4)\n    conv6 = Dropout(0.3)(conv5)\n    conv6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv5)\n\n    up1 = concatenate([UpSampling2D((2, 2))(conv5), conv3], axis=-1)\n    conv6 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(up1)\n    conv6 = Dropout(0.3)(conv6)\n    conv6 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv6)\n    \n    up2 = concatenate([UpSampling2D((2, 2))(conv5), conv2], axis=-1)\n    conv7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(up2)\n    conv7 = Dropout(0.3)(conv6)\n    conv7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv6)\n\n    up3 = concatenate([UpSampling2D((2, 2))(conv6), conv1], axis=-1)\n    conv8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(up3)\n    conv8 = Dropout(0.3)(conv7)\n    conv8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv8)\n    segmentation = Conv2D(1, (1, 1), activation='sigmoid', name='seg')(conv8)\n\n    model = Model(inputs=[in1], outputs=[segmentation])\n    print(model.summary())\n    \n    optimizer=keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=0.001, decay=0.0, amsgrad=True)\n    \n    model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['acc', Dice_coeff])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:13:40.668337Z","iopub.execute_input":"2021-09-10T05:13:40.668819Z","iopub.status.idle":"2021-09-10T05:13:40.686944Z","shell.execute_reply.started":"2021-09-10T05:13:40.668782Z","shell.execute_reply":"2021-09-10T05:13:40.685866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_model = unet_model()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:13:40.688652Z","iopub.execute_input":"2021-09-10T05:13:40.689035Z","iopub.status.idle":"2021-09-10T05:13:43.030868Z","shell.execute_reply.started":"2021-09-10T05:13:40.688999Z","shell.execute_reply":"2021-09-10T05:13:43.030047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model checkpoint and Early Stopping","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping,ModelCheckpoint\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\nmc = ModelCheckpoint(SAVE_DIR+'Segmentor_checkpoint.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:13:43.033853Z","iopub.execute_input":"2021-09-10T05:13:43.034166Z","iopub.status.idle":"2021-09-10T05:13:43.038373Z","shell.execute_reply.started":"2021-09-10T05:13:43.034135Z","shell.execute_reply":"2021-09-10T05:13:43.037538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unet_train_X = []\nunet_train_Y = []\n\nfor img_id in tqdm(unet_train_ids):\n    train_img, msk_img = load_images(img_id)\n    unet_train_X.append(train_img)\n    unet_train_Y.append(msk_img)\nunet_train_X = np.array(unet_train_X)\nunet_train_Y = np.array(unet_train_Y)\nprint('Training done')","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:13:43.039717Z","iopub.execute_input":"2021-09-10T05:13:43.040046Z","iopub.status.idle":"2021-09-10T05:23:38.182048Z","shell.execute_reply.started":"2021-09-10T05:13:43.040013Z","shell.execute_reply":"2021-09-10T05:23:38.181073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the model","metadata":{}},{"cell_type":"code","source":"num_epoch = 60\nbatch_size = 30\nn_points = len(unet_train_X)\n\nhistory = seq_model.fit(x=unet_train_X, y=unet_train_Y, \n                validation_split=0.15,\n                epochs=num_epoch,steps_per_epoch = np.ceil(n_points / batch_size), callbacks =[es,mc],  shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:23:38.183319Z","iopub.execute_input":"2021-09-10T05:23:38.183675Z","iopub.status.idle":"2021-09-10T05:30:40.148946Z","shell.execute_reply.started":"2021-09-10T05:23:38.183645Z","shell.execute_reply":"2021-09-10T05:30:40.146881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_model.predict(unet_train_X[:2])","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:30:40.153549Z","iopub.execute_input":"2021-09-10T05:30:40.153819Z","iopub.status.idle":"2021-09-10T05:30:40.570369Z","shell.execute_reply.started":"2021-09-10T05:30:40.15379Z","shell.execute_reply":"2021-09-10T05:30:40.569607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Gradcam","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:31:26.257088Z","iopub.execute_input":"2021-09-10T05:31:26.257426Z","iopub.status.idle":"2021-09-10T05:31:26.261313Z","shell.execute_reply.started":"2021-09-10T05:31:26.257394Z","shell.execute_reply":"2021-09-10T05:31:26.260392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install timm","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:32:25.139757Z","iopub.execute_input":"2021-09-10T05:32:25.140085Z","iopub.status.idle":"2021-09-10T05:32:34.365924Z","shell.execute_reply.started":"2021-09-10T05:32:25.140056Z","shell.execute_reply":"2021-09-10T05:32:34.36494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport timm\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torchvision import transforms\n\nplt.rcParams[\"figure.figsize\"] = (20,20)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:32:36.937318Z","iopub.execute_input":"2021-09-10T05:32:36.937722Z","iopub.status.idle":"2021-09-10T05:32:39.212377Z","shell.execute_reply.started":"2021-09-10T05:32:36.937683Z","shell.execute_reply":"2021-09-10T05:32:39.211472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureExtractor():\n    \"\"\" Class for extracting activations and\n    registering gradients from targetted intermediate layers \"\"\"\n\n    def __init__(self, model, target_layers):\n        self.model = model\n        self.target_layers = target_layers\n        self.gradients = []\n\n    def save_gradient(self, grad):\n        self.gradients.append(grad)\n\n    def __call__(self, x):\n        outputs = []\n        self.gradients = []\n        for name, module in self.model._modules.items():\n            x = module(x)\n            if name in self.target_layers:\n                x.register_hook(self.save_gradient)\n                outputs += [x]\n        return outputs, x\n\nclass ModelOutputs():\n    \"\"\" Class for making a forward pass, and getting:\n    1. The network output.\n    2. Activations from intermeddiate targetted layers.\n    3. Gradients from intermeddiate targetted layers. \"\"\"\n\n    def __init__(self, model, feature_module, target_layers):\n        self.model = model\n        self.feature_module = feature_module\n        self.feature_extractor = FeatureExtractor(self.feature_module, target_layers)\n\n    def get_gradients(self):\n        return self.feature_extractor.gradients\n\n    def __call__(self, x):\n        target_activations = []\n        for name, module in self.model._modules.items():\n            if module == self.feature_module:\n                target_activations, x = self.feature_extractor(x)\n            elif \"avgpool\" in name.lower():\n                x = module(x)\n                x = x.view(x.size(0),-1)\n            else:\n                x = module(x)\n\n        return target_activations, x\n\ndef preprocess_image(img):\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n    preprocessing = transforms.Compose([\n        transforms.ToTensor(),\n        normalize,\n    ])\n    return preprocessing(img.copy()).unsqueeze(0)\n\ndef show_cam_on_image(img, mask):\n    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n    heatmap = np.float32(heatmap) / 255\n    cam = heatmap + np.float32(img)\n    cam = cam / np.max(cam)\n    return np.uint8(255 * cam)\n\nclass GradCam:\n    def __init__(self, model, feature_module, target_layer_names, use_cuda):\n        self.model = model\n        self.feature_module = feature_module\n        self.model.eval()\n        self.cuda = use_cuda\n        if self.cuda:\n            self.model = model.cuda()\n\n        self.extractor = ModelOutputs(self.model, self.feature_module, target_layer_names)\n\n    def forward(self, input_img):\n        return self.model(input_img)\n\n    def __call__(self, input_img, target_category=None):\n        if self.cuda:\n            input_img = input_img.cuda()\n\n        features, output = self.extractor(input_img)\n\n        if target_category == None:\n            target_category = np.argmax(output.cpu().data.numpy())\n\n        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n        one_hot[0][target_category] = 1\n        one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n        if self.cuda:\n            one_hot = one_hot.cuda()\n        \n        one_hot = torch.sum(one_hot * output)\n\n        self.feature_module.zero_grad()\n        self.model.zero_grad()\n        one_hot.backward(retain_graph=True)\n\n        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n\n        target = features[-1]\n        target = target.cpu().data.numpy()[0, :]\n\n        weights = np.mean(grads_val, axis=(2, 3))[0, :]\n        cam = np.zeros(target.shape[1:], dtype=np.float32)\n\n        for i, w in enumerate(weights):\n            cam += w * target[i, :, :]\n\n        cam = np.maximum(cam, 0)\n        cam = cv2.resize(cam, input_img.shape[2:])\n        cam = cam - np.min(cam)\n        cam = cam / np.max(cam)\n        return cam\n\n\nclass GuidedBackpropReLU(Function):\n    @staticmethod\n    def forward(self, input_img):\n        positive_mask = (input_img > 0).type_as(input_img)\n        output = torch.addcmul(torch.zeros(input_img.size()).type_as(input_img), input_img, positive_mask)\n        self.save_for_backward(input_img, output)\n        return output\n\n    @staticmethod\n    def backward(self, grad_output):\n        input_img, output = self.saved_tensors\n        grad_input = None\n\n        positive_mask_1 = (input_img > 0).type_as(grad_output)\n        positive_mask_2 = (grad_output > 0).type_as(grad_output)\n        grad_input = torch.addcmul(torch.zeros(input_img.size()).type_as(input_img),\n                                   torch.addcmul(torch.zeros(input_img.size()).type_as(input_img), grad_output,\n                                                 positive_mask_1), positive_mask_2)\n        return grad_input\n\n\nclass GuidedBackpropReLUModel:\n    def __init__(self, model, use_cuda):\n        self.model = model\n        self.model.eval()\n        self.cuda = use_cuda\n        if self.cuda:\n            self.model = model.cuda()\n\n        def recursive_relu_apply(module_top):\n            for idx, module in module_top._modules.items():\n                recursive_relu_apply(module)\n                if module.__class__.__name__ == 'ReLU':\n                    module_top._modules[idx] = GuidedBackpropReLU.apply\n\n        # replace ReLU with GuidedBackpropReLU\n        recursive_relu_apply(self.model)\n\n    def forward(self, input_img):\n        return self.model(input_img)\n\n    def __call__(self, input_img, target_category=None):\n        if self.cuda:\n            input_img = input_img.cuda()\n\n        input_img = input_img.requires_grad_(True)\n\n        output = self.forward(input_img)\n\n        if target_category == None:\n            target_category = np.argmax(output.cpu().data.numpy())\n\n        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n        one_hot[0][target_category] = 1\n        one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n        if self.cuda:\n            one_hot = one_hot.cuda()\n\n        one_hot = torch.sum(one_hot * output)\n        one_hot.backward(retain_graph=True)\n\n        output = input_img.grad.cpu().data.numpy()\n        output = output[0, :, :, :]\n\n        return output\n\ndef deprocess_image(img):\n    \"\"\" see https://github.com/jacobgil/keras-grad-cam/blob/master/grad-cam.py#L65 \"\"\"\n    img = img - np.mean(img)\n    img = img / (np.std(img) + 1e-5)\n    img = img * 0.1\n    img = img + 0.5\n    img = np.clip(img, 0, 1)\n    return np.uint8(img*255)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:33:12.302704Z","iopub.execute_input":"2021-09-10T05:33:12.303088Z","iopub.status.idle":"2021-09-10T05:33:12.33609Z","shell.execute_reply.started":"2021-09-10T05:33:12.303048Z","shell.execute_reply":"2021-09-10T05:33:12.335175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" \n1. Loads an image with opencv.\n2. Preprocesses it and converts to a pytorch variable.\n3. Makes a forward pass to find the category index with the highest score,\nand computes intermediate activations.\nMakes the visualization. \n\n\"\"\"\nfrom PIL import Image\nIMAGE_PATH = \"../input/prostate-cancer-grade-assessment/train_images/001c62abd11fa4b57bf7a6c603a11bb9.tiff\"\nim = Image.open(\"../input/prostate-cancer-grade-assessment/train_images/001c62abd11fa4b57bf7a6c603a11bb9.tiff\")\n\n\n\nimg = np.float32(img) / 255\n# # Opencv loads as BGR:\n# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\ninput_img = preprocess_image(img)\n\ngrad_cam = GradCam(model=seq_model, feature_module=model.blocks,\n                   target_layer_names=[\"conv2d_13\"], use_cuda=use_cuda)\n\n# If None, returns the map for the highest scoring category.\n# Otherwise, targets the requested category.\ntarget_category = None\ngrayscale_cam = grad_cam(input_img, target_category)\n\ngrayscale_cam = cv2.resize(grayscale_cam, (img.shape[1], img.shape[0]))\ncam = show_cam_on_image(img, grayscale_cam)\n\ngb_model = GuidedBackpropReLUModel(model=seq_model, use_cuda=use_cuda)\ngb = gb_model(input_img, target_category=target_category)\ngb = gb.transpose((1, 2, 0))\n\ncam_mask = cv2.merge([grayscale_cam, grayscale_cam, grayscale_cam])\ncam_gb = deprocess_image(cam_mask*gb)\ngb = deprocess_image(gb)\n\ncv2.imwrite(\"cam.jpg\", cam)\ncv2.imwrite('gb.jpg', gb)\ncv2.imwrite('cam_gb.jpg', cam_gb);","metadata":{"execution":{"iopub.status.busy":"2021-09-10T06:01:16.44808Z","iopub.execute_input":"2021-09-10T06:01:16.448446Z","iopub.status.idle":"2021-09-10T06:01:16.482941Z","shell.execute_reply.started":"2021-09-10T06:01:16.448408Z","shell.execute_reply":"2021-09-10T06:01:16.481687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for image in base_data['image_id']:\n    if os.path.exists(BASE_DIR+'train_images/'+image+'.tiff'):\n        print(BASE_DIR+'train_images/'+image+'.tiff')","metadata":{"execution":{"iopub.status.busy":"2021-09-10T05:51:45.671301Z","iopub.execute_input":"2021-09-10T05:51:45.671688Z","iopub.status.idle":"2021-09-10T05:51:49.80325Z","shell.execute_reply.started":"2021-09-10T05:51:45.671655Z","shell.execute_reply":"2021-09-10T05:51:49.802021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Overlay the mask on the images","metadata":{}},{"cell_type":"code","source":"def overlay_mask_on_slide(image_id, alpha=0.8, max_size=(IMG_SIZE, IMG_SIZE)):\n    \n    slide = openslide.OpenSlide(os.path.join(BASE_DIR+\"train_images\", f'{image_id}.tiff'))\n    mask = openslide.OpenSlide(os.path.join(BASE_DIR+\"train_label_masks\", f'{image_id}_mask.tiff'))\n#     mask = seq_model.predict(x=[slide])\n\n    slide_data = slide.read_region((0,0), slide.level_count - 1, slide.level_dimensions[-1])\n    mask_data = mask.read_region((0,0), mask.level_count - 1, mask.level_dimensions[-1])\n    mask_data = mask_data.split()[0]\n\n    alpha_int = int(round(255*alpha))\n    alpha_content = np.less(mask_data.split()[0], 2).astype('uint8') * alpha_int + (255 - alpha_int)\n\n    alpha_content = PIL.Image.fromarray(alpha_content)\n    preview_palette = np.zeros(shape=768, dtype=int)\n\n    preview_palette[0:18] = (np.array([0, 0, 0, 0.5, 0.5, 0.5, 0, 1, 0, 1, 1, 0.7, 1, 0.5, 0, 1, 0, 0]) * 255).astype(int)\n    \n    mask_data.putpalette(data=preview_palette.tolist())\n    mask_rgb = mask_data.convert(mode='RGB')\n    overlayed_image = PIL.Image.composite(image1=slide_data, image2=mask_rgb, mask=alpha_content)\n    overlayed_image.thumbnail(size=max_size, resample=0)\n\n    overlayed_image = overlayed_image.resize(max_size)\n\n    return overlayed_image","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:54:09.948352Z","iopub.execute_input":"2021-09-07T12:54:09.948673Z","iopub.status.idle":"2021-09-07T12:54:09.957837Z","shell.execute_reply.started":"2021-09-07T12:54:09.948644Z","shell.execute_reply":"2021-09-07T12:54:09.957059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating directories for the images\noverlay_train_img_folder = OVERLAY_IMG_DIR+'train/'\noverlay_test_img_folder = OVERLAY_IMG_DIR+'test/'\n\nif not os.path.exists(overlay_train_img_folder):\n    os.makedirs(overlay_train_img_folder)\n\nif not os.path.exists(overlay_test_img_folder):\n    os.makedirs(overlay_test_img_folder)\n\n#A directory for each label\nlabels = unet_train_labels.unique()\nfor lbl in labels:\n    if not os.path.exists(overlay_train_img_folder+str(lbl)):\n        os.makedirs(overlay_train_img_folder+str(lbl))\n    \n    if not os.path.exists(overlay_test_img_folder+str(lbl)):\n        os.makedirs(overlay_test_img_folder+str(lbl))","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:53:46.202268Z","iopub.execute_input":"2021-09-07T12:53:46.202596Z","iopub.status.idle":"2021-09-07T12:53:46.211749Z","shell.execute_reply.started":"2021-09-07T12:53:46.202567Z","shell.execute_reply":"2021-09-07T12:53:46.210681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, img_id in tqdm(enumerate(unet_train_ids)):\n    overlay_image = overlay_mask_on_slide(image_id=img_id)\n    file_name = overlay_train_img_folder+'/'+str(unet_train_labels.iloc[i])+'/'+img_id+'.jpeg'\n    overlay_image.save(file_name)\n\nfor i, img_id in tqdm(enumerate(unet_test_ids)):\n    overlay_image = overlay_mask_on_slide(image_id=img_id)\n    file_name = overlay_test_img_folder+'/'+str(unet_test_labels.iloc[i])+'/'+img_id+'.jpeg'\n    overlay_image.save(file_name)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:54:13.906784Z","iopub.execute_input":"2021-09-07T12:54:13.90716Z","iopub.status.idle":"2021-09-07T13:04:33.354959Z","shell.execute_reply.started":"2021-09-07T12:54:13.90713Z","shell.execute_reply":"2021-09-07T13:04:33.353866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Tiling of the overlay images","metadata":{}},{"cell_type":"code","source":"tiling_train_img_data = TILING_IMG_DIR+'train/'\ntiling_test_img_data = TILING_IMG_DIR+'test/'\n\nif not os.path.exists(tiling_train_img_data):\n    os.makedirs(tiling_train_img_data)\n\nif not os.path.exists(tiling_test_img_data):\n    os.makedirs(tiling_test_img_data)\n    \nlabels = unet_train_labels.unique()\nfor lbl in labels:\n    if not os.path.exists(tiling_train_img_data+str(lbl)):\n        os.makedirs(tiling_train_img_data+str(lbl))\n    \n    if not os.path.exists(tiling_test_img_data+str(lbl)):\n        os.makedirs(tiling_test_img_data+str(lbl))\n\n# Parameters for cropping images\ncropPx= 56\ncropN = 16\nassert np.sqrt(cropN) == round(np.sqrt(cropN))","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:14:15.043033Z","iopub.execute_input":"2021-09-07T13:14:15.043361Z","iopub.status.idle":"2021-09-07T13:14:15.052098Z","shell.execute_reply.started":"2021-09-07T13:14:15.043332Z","shell.execute_reply":"2021-09-07T13:14:15.051102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tile(img):\n    result = []\n    shape = img.shape\n    pad0,pad1 = (cropPx - shape[0]%cropPx)%cropPx, (cropPx - shape[1]%cropPx)%cropPx\n    img = np.pad(img,[[pad0//2,pad0-pad0//2],[pad1//2,pad1-pad1//2],[0,0]],\n                constant_values=255)\n    \n    img = img.reshape(img.shape[0]//cropPx,cropPx,img.shape[1]//cropPx,cropPx,3)\n    img = img.transpose(0,2,1,3,4).reshape(-1,cropPx,cropPx,3)\n    \n    if len(img) < cropN:\n        img = np.pad(img,[[0,cropN-len(img)],[0,0],[0,0],[0,0]],constant_values=255)\n    idxs = np.argsort(img.reshape(img.shape[0],-1).sum(-1))[:cropN]\n    img = img[idxs]\n    for i in range(len(img)):\n        result.append({'img':img[i], 'idx':i})\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:14:20.417078Z","iopub.execute_input":"2021-09-07T13:14:20.417399Z","iopub.status.idle":"2021-09-07T13:14:20.42689Z","shell.execute_reply.started":"2021-09-07T13:14:20.41737Z","shell.execute_reply":"2021-09-07T13:14:20.425656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(os.listdir(overlay_test_img_folder+str(label)))","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:19:38.75586Z","iopub.execute_input":"2021-09-07T13:19:38.756217Z","iopub.status.idle":"2021-09-07T13:19:38.762777Z","shell.execute_reply.started":"2021-09-07T13:19:38.756187Z","shell.execute_reply":"2021-09-07T13:19:38.761934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nbCol = int(np.sqrt(cropN))\n\nlabels = os.listdir(overlay_train_img_folder)\nfor label in tqdm(labels):\n    for name in tqdm(os.listdir(overlay_train_img_folder+str(label))):\n        img = skimage.io.MultiImage(os.path.join(overlay_train_img_folder+str(label)+'/',name))[-1]\n        tiles = tile(img)\n        stackImg = np.vstack([np.hstack([tiles[nbCol*col + row]['img'] for row in range(nbCol)])\n                   for col in range(nbCol)])\n        cv2.imwrite(tiling_train_img_data+str(label)+'/'+name, stackImg)\n\n\nnames = os.listdir(overlay_test_img_folder)\nfor label in tqdm(labels):\n    for name in tqdm(os.listdir(overlay_test_img_folder+str(label))):\n        img = skimage.io.MultiImage(os.path.join(overlay_test_img_folder+str(label)+'/',name))[-1]\n        tiles = tile(img)\n        stackImg = np.vstack([np.hstack([tiles[nbCol*col + row]['img'] for row in range(nbCol)])\n                   for col in range(nbCol)])\n        cv2.imwrite(tiling_test_img_data+str(label)+'/'+name, stackImg)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:20:04.593416Z","iopub.execute_input":"2021-09-07T13:20:04.593761Z","iopub.status.idle":"2021-09-07T13:20:06.674759Z","shell.execute_reply.started":"2021-09-07T13:20:04.593731Z","shell.execute_reply":"2021-09-07T13:20:06.673692Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"overlay_test_img_folder+'0/'+os.listdir(overlay_test_img_folder+'0')[0]","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:29:32.544156Z","iopub.execute_input":"2021-09-07T13:29:32.544483Z","iopub.status.idle":"2021-09-07T13:29:32.551147Z","shell.execute_reply.started":"2021-09-07T13:29:32.544455Z","shell.execute_reply":"2021-09-07T13:29:32.550173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(overlay_test_img_folder)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:30:42.994035Z","iopub.execute_input":"2021-09-07T13:30:42.994362Z","iopub.status.idle":"2021-09-07T13:30:43.001025Z","shell.execute_reply.started":"2021-09-07T13:30:42.994333Z","shell.execute_reply":"2021-09-07T13:30:43.000013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open(overlay_test_img_folder+'0/'+os.listdir(overlay_test_img_folder+'0')[0])\nimg = img.resize((IMG_SIZE, IMG_SIZE))\nimg = np.array(img)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:32:15.018927Z","iopub.execute_input":"2021-09-07T13:32:15.01925Z","iopub.status.idle":"2021-09-07T13:32:15.026544Z","shell.execute_reply.started":"2021-09-07T13:32:15.019221Z","shell.execute_reply":"2021-09-07T13:32:15.02558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:32:24.185897Z","iopub.execute_input":"2021-09-07T13:32:24.186272Z","iopub.status.idle":"2021-09-07T13:32:24.399737Z","shell.execute_reply.started":"2021-09-07T13:32:24.186241Z","shell.execute_reply":"2021-09-07T13:32:24.398824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Classifier","metadata":{}},{"cell_type":"markdown","source":"### Image data Generator for the model","metadata":{}},{"cell_type":"code","source":" image_gen = ImageDataGenerator(\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    rescale=1/255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode=\"nearest\"\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:34:07.183527Z","iopub.execute_input":"2021-09-07T13:34:07.183851Z","iopub.status.idle":"2021-09-07T13:34:07.18845Z","shell.execute_reply.started":"2021-09-07T13:34:07.183823Z","shell.execute_reply":"2021-09-07T13:34:07.187588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\ntrain_image_gen = image_gen.flow_from_directory(tiling_train_img_data,\n                                                target_size=(IMG_SIZE, IMG_SIZE),\n                                                batch_size=batch_size,\n                                                class_mode=\"categorical\")","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:34:09.828388Z","iopub.execute_input":"2021-09-07T13:34:09.82873Z","iopub.status.idle":"2021-09-07T13:34:09.943551Z","shell.execute_reply.started":"2021-09-07T13:34:09.828699Z","shell.execute_reply":"2021-09-07T13:34:09.942654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_image_gen = image_gen.flow_from_directory(tiling_test_img_data,\n                                                target_size=(IMG_SIZE, IMG_SIZE),\n                                                batch_size=batch_size,\n                                                class_mode=\"categorical\")","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:34:11.07317Z","iopub.execute_input":"2021-09-07T13:34:11.0735Z","iopub.status.idle":"2021-09-07T13:34:11.184797Z","shell.execute_reply.started":"2021-09-07T13:34:11.073472Z","shell.execute_reply":"2021-09-07T13:34:11.183844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The classifier model - Xception","metadata":{}},{"cell_type":"code","source":"model_builder = keras.applications.xception.Xception\n\nmodel = models.Sequential()\nmodel.add(model_builder(include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3), pooling='avg'))\nmodel.add(Dense(6, activation='softmax'))\n\n# optimizer=keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=0.001, decay=0.0, amsgrad=True)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=optimizers.RMSprop(lr=3e-4),\n    metrics=[\"acc\"],\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T08:10:34.606356Z","iopub.execute_input":"2021-09-10T08:10:34.606694Z","iopub.status.idle":"2021-09-10T08:10:36.731914Z","shell.execute_reply.started":"2021-09-10T08:10:34.606662Z","shell.execute_reply":"2021-09-10T08:10:36.730145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping,ModelCheckpoint\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\nmc = ModelCheckpoint(SAVE_DIR+'Classifier_checkpoint.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:34:40.676578Z","iopub.execute_input":"2021-09-07T13:34:40.676893Z","iopub.status.idle":"2021-09-07T13:34:40.683452Z","shell.execute_reply.started":"2021-09-07T13:34:40.676865Z","shell.execute_reply":"2021-09-07T13:34:40.682493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUMBER_OF_TRAINING_IMAGES = unet_train_ids.shape[0]\nNUMBER_OF_TESTING_IMAGES = unet_test_ids.shape[0]\nresults = model.fit(\n    train_image_gen,\n    steps_per_epoch=NUMBER_OF_TRAINING_IMAGES // batch_size,\n    epochs=50,\n    validation_data=test_image_gen,\n    validation_steps=NUMBER_OF_TESTING_IMAGES // batch_size,\n    verbose=1,\n    use_multiprocessing=True,\n    callbacks =[es,mc],\n    workers=4,\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:35:18.76862Z","iopub.execute_input":"2021-09-07T13:35:18.768956Z","iopub.status.idle":"2021-09-07T13:56:34.012819Z","shell.execute_reply.started":"2021-09-07T13:35:18.768924Z","shell.execute_reply":"2021-09-07T13:56:34.01177Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-09-07T14:14:46.315212Z","iopub.execute_input":"2021-09-07T14:14:46.315539Z","iopub.status.idle":"2021-09-07T14:14:46.339992Z","shell.execute_reply.started":"2021-09-07T14:14:46.31551Z","shell.execute_reply":"2021-09-07T14:14:46.338228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open(tiling_test_img_data+'2/'+os.listdir(overlay_test_img_folder+'2')[1])\nimg = img.resize((IMG_SIZE, IMG_SIZE))\nimg = np.array(img)\n\nimg = np.expand_dims(img, axis=0)\nmodel.predict(img)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T14:14:27.347641Z","iopub.execute_input":"2021-09-07T14:14:27.347995Z","iopub.status.idle":"2021-09-07T14:14:27.401558Z","shell.execute_reply.started":"2021-09-07T14:14:27.347963Z","shell.execute_reply":"2021-09-07T14:14:27.400784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-07T14:11:02.098203Z","iopub.execute_input":"2021-09-07T14:11:02.09853Z","iopub.status.idle":"2021-09-07T14:11:02.104013Z","shell.execute_reply.started":"2021-09-07T14:11:02.0985Z","shell.execute_reply":"2021-09-07T14:11:02.103106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T14:10:55.566312Z","iopub.execute_input":"2021-09-07T14:10:55.566646Z","iopub.status.idle":"2021-09-07T14:10:55.58577Z","shell.execute_reply.started":"2021-09-07T14:10:55.566617Z","shell.execute_reply":"2021-09-07T14:10:55.584832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-09-07T14:12:18.643149Z","iopub.execute_input":"2021-09-07T14:12:18.643534Z","iopub.status.idle":"2021-09-07T14:12:19.492635Z","shell.execute_reply.started":"2021-09-07T14:12:18.643501Z","shell.execute_reply":"2021-09-07T14:12:19.491839Z"},"trusted":true},"execution_count":null,"outputs":[]}]}